{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtoj1bVLA0mGTsaBV6Krg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanshu88/Sentiment-Analysis-Prediction/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGuA-C90xZeG",
        "outputId": "522d918d-adae-4157-bf57-5a4c152394cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "irwH9MP-SMYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1714af30-43d3-44c3-fd79-859899fc8101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "Hit Enter to continue: \n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "Hit Enter to continue: \n",
            "Hit Enter to continue: \n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "Hit Enter to continue: \n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "Hit Enter to continue: \n",
            "  Identifier> \n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> x\n"
          ]
        }
      ],
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import nltk,re,string\n",
        "nltk.download()\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing of the tweets that is our data\n",
        "nltk.download('stopwords')\n",
        "def process_tweet(tweet):\n",
        "  stemmer = nltk.PorterStemmer()\n",
        "  stopwords_english = stopwords.words(\"english\")\n",
        "  tweet = re.sub(r\"\\$\\w*\",\"\", tweet)\n",
        "  tweet = re.sub(r\"^RT[\\s]+\",\"\",tweet)\n",
        "  tweet = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\",\"\",tweet)\n",
        "  tweet = re.sub(r\"#\",\"\",tweet)\n",
        "  tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles = True, reduce_len = True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if(word not in stopwords_english and word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word) # Stemming word\n",
        "      tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean\n"
      ],
      "metadata": {
        "id": "tGsM8Qh_S5jI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b98da9-fb26-4690-e836-cc42d242c1f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our feature set through which we will train our model wll be build here."
      ],
      "metadata": {
        "id": "j28Pko-aV5wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  \"\"\"\n",
        "  Build frequencies.\n",
        "  Input:\n",
        "    tweets: a list of tweets\n",
        "    ys: an m*1 array with the sentiment label of each tweet (either 0 or 1)\n",
        "\n",
        "  Output:\n",
        "    freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert np array to list since zip needs an iterable.\n",
        "  # The squeeze is necessary or the list ends up with one element.\n",
        "  # Also note that this is just a NOP if ys is already a list.\n",
        "\n",
        "  yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "  # Start with an empty dictionary and populate it by looping over all tweets\n",
        "  # and over all processed words in each tweet.\n",
        "\n",
        "  freqs={}\n",
        "  for y,tweet in zip(yslist,tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word,y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] += 1\n",
        "      else:\n",
        "        freqs[pair]=1\n",
        "  \n",
        "  return freqs\n"
      ],
      "metadata": {
        "id": "JLROu2N0nwo2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how the above code works with an example.\n",
        "\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "res = build_freqs(tweets, ys)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiGX4obGsLMA",
        "outputId": "2309d6c9-8ae7-457e-a686-010870f7962c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Q36mzEuKUn",
        "outputId": "5d395ac5-f929-4251-aad1-926eec229992"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the set of positive and negative tweets\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "f-tA-h1fzfHQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into two pieces, one for training and one for testing.\n",
        "\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n"
      ],
      "metadata": {
        "id": "iBk5m-F6zzp5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_pos + train_neg\n",
        "test_x  = test_pos + test_neg"
      ],
      "metadata": {
        "id": "ApJnWbsQ0HVw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine positive and negative labels\n",
        "# We are building our y - target variable here\n",
        "\n",
        "train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)),axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis = 0)"
      ],
      "metadata": {
        "id": "x4VJNwW60q48"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create frequency dictionary\n",
        "\n",
        "freqs = build_freqs(train_x,train_y)"
      ],
      "metadata": {
        "id": "u40-S56k1KAk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the output\n",
        "\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyOD7o2_1TZY",
        "outputId": "33ad0de7-abcf-4610-fa0b-535d2682fdf9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function below\n",
        "\n",
        "print(\"This is an example of a positive tweet: \\n\", train_x[22])\n",
        "print(\"\\nThis is an example of the processed version of the tweet: \\n\", process_tweet(train_x[22]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWrAXr7D1qyy",
        "outputId": "0439fd7c-aba0-4de1-97bc-1c7522c30a72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " @gculloty87 Yeah I suppose she was lol! Chat in a bit just off out x :))\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unlike most, we will actully build a Logistic Regression Model from scratch\n",
        "# Logistic regression\n",
        "\n",
        "# Sigmoid Function\n",
        "\n",
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Input: \n",
        "    z: is the input (can be a scalar or an array)\n",
        "  Output:\n",
        "    h: the sigmoid of z\n",
        "  \"\"\"\n",
        "\n",
        "  zz = np.negative(z)\n",
        "  h = 1 / (1 + np.exp(zz))\n",
        "  return h"
      ],
      "metadata": {
        "id": "mJUm8yK32R0l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost function and Gradient\n",
        "\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    x: matrix of features which is (m,n+1)\n",
        "    y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "    theta: weight vector of dimension (n+1,1)\n",
        "    alpha: learning rate\n",
        "    num_iters: number of iterations you want to train your model for\n",
        "  Output:\n",
        "    J: the final cost\n",
        "    theta: your final weight vector\n",
        "    print the cost to make sure that it is going down.\n",
        "    \"\"\"\n",
        "  # Get 'm', the number of rows in matrix x\n",
        "\n",
        "  m = x.shape[0]\n",
        "  for i in range(0, num_iters):\n",
        "    z = np.dot(x, theta)\n",
        "    h = sigmoid(z)\n",
        "\n",
        "    # Calculate the cost function\n",
        "\n",
        "    cost = -1. / m * (np.dot(y.transpose(),np.log(h)) + np.dot((1-y).transpose(), np.log(1-h)))\n",
        "\n",
        "    # Update the weights theta\n",
        "\n",
        "    theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n",
        "\n",
        "  cost = float(cost)\n",
        "  return cost, theta"
      ],
      "metadata": {
        "id": "0DJ6Itc42vn8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the features\n",
        "\n",
        "def extract_features(tweet, freqs):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "  \"\"\"\n",
        "\n",
        "  word_l = process_tweet(tweet)\n",
        "  x = np.zeros((1,3))\n",
        "\n",
        "  # Bias term is set to 1\n",
        "\n",
        "  x[0, 0] = 1\n",
        "\n",
        "  for word in word_l:\n",
        "\n",
        "    # Increment the word count for the positive label 1\n",
        "    x[0, 1] += freqs.get((word, 1.0), 0)\n",
        "\n",
        "    # Increment the word count for the negative label 0    \n",
        "    x[0, 2] += freqs.get((word, 0.0), 0)\n",
        "\n",
        "  assert(x.shape == (1, 3))\n",
        "  return x"
      ],
      "metadata": {
        "id": "BL_pyMZV38OR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on training data\n",
        "\n",
        "tmp1 = extract_features(train_x[22],freqs)\n",
        "print(tmp1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCM1h23I449e",
        "outputId": "30becb87-0ece-46e2-dd80-da70a80eea2f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.006e+03 1.240e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Your Model\n",
        "\n",
        "# Collecting the features 'x' and stack them into a matrix 'X'\n",
        "\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "# Training labels corresponding to X\n",
        "\n",
        "Y = train_y\n",
        "\n",
        "# Applying gradient descent\n",
        "# These values are predefined\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)"
      ],
      "metadata": {
        "id": "4hPQbBuZ5Cm7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "  \"\"\"\n",
        "  # Extract the features of the tweet and store it into x\n",
        "  x = extract_features(tweet, freqs)\n",
        "  y_pred = sigmoid(np.dot(x, theta))\n",
        "\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "V8Uy8USN6BRP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "  \"\"\"\n",
        "\n",
        "  # The list for storing predictions\n",
        "\n",
        "  y_hat = []\n",
        "\n",
        "  for tweet in test_x:\n",
        "  # Get the label prediction for the tweet\n",
        "\n",
        "    y_pred = predict_tweet(tweet, freqs, theta)\n",
        "    if y_pred > 0.5:\n",
        "      y_hat.append(1)\n",
        "    else:\n",
        "      y_hat.append(0)\n",
        "\n",
        "  accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "  return accuracy \n"
      ],
      "metadata": {
        "id": "WLH-T-AA6d0a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8_eE5dL7jzQ",
        "outputId": "0ccfdfc1-2e0e-4c88-8923-fdb2e86a4652"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with your own tweet\n",
        "\n",
        "def pre(sentence):\n",
        "  yhat = predict_tweet(sentence, freqs, theta)\n",
        "  if yhat > 0.5:\n",
        "    return \"Positive sentiment\"\n",
        "  elif yhat == 0:\n",
        "    return \"Natural sentiment\"\n",
        "  else:\n",
        "    return \"Negative sentiment\"\n"
      ],
      "metadata": {
        "id": "Q5uxquMl7-KY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet = \"It is so hot today but it is the perfect day for a beach party\"\n",
        "\n",
        "res = pre(my_tweet)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi7DzLxh8a9o",
        "outputId": "43eda380-0ed9-4e81-9c7b-f578645a3947"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment\n"
          ]
        }
      ]
    }
  ]
}